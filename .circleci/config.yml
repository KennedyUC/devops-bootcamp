version: 2.1

parameters: 
  app_namespace:
    type: string
    default: test_app
  eks_cluster:
    type: string
    default: test_cluster
  aws_region:
    type: string
    default: us-east-1

commands:
  install_awscli:
    description: Install AWSCLI, Tar, GZIP, Kubectl, yq & Curl
    steps:
      - run:
          name: Install AWSCLI, Tar, GZIP, Kubectl, yq & Curl
          command: |
            sudo apt install -y tar gzip curl
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install

            curl -o kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
            chmod +x kubectl
            sudo mv kubectl /usr/local/bin/

            sudo apt-get update
            sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
            sudo chmod a+x /usr/local/bin/yq

            aws --version
            kubectl version --client

  destroy-eks:
    description: Destroy the EKS resources given a workflow ID.
    steps:
      - run:
          name: Destroy EKS Resources
          when: on_fail
          command: 
            echo Destroying EKS Resources for ${CIRCLE_WORKFLOW_ID:0:7}
            terraform destroy -auto-approve -var-file="vars.tfvars" -var="user_access_key=${AWS_ACCESS_KEY}" \
                              -var="user_secret_key=${AWS_SECRET_KEY}" -var="region=${aws_region}" \
                              -var="state_file_path=${STATE_FILE_PATH}" \
                              -var="k8s_cluster_name=${eks_cluster}"
            
jobs:
  build-frontend:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - run:
          name: Install Make
          command: |
            sudo apt-get update
            sudo apt-get install make -y
            make --version
      - run:
          name: Build Frontend Image
          command: |
            make docker-login DOCKER_REPO=${DOCKER_REPO} DOCKER_PASSWORD=${DOCKER_PASSWORD}
            make build-frontend IMAGE_TAG=${CIRCLE_WORKFLOW_ID:0:7}

  build-backend:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - run:
          name: Install Docker and Make
          command: |
            sudo apt-get update
            sudo apt-get install make -y
            make --version
      - run:
          name: Build Backend Image
          command: |
            make docker-login DOCKER_REPO=${DOCKER_REPO} DOCKER_PASSWORD=${DOCKER_PASSWORD}
            make build-backend IMAGE_TAG=${CIRCLE_WORKFLOW_ID:0:7}
      - run:
          name: Build PostgresQL Database Image
          command: |
            make build-db IMAGE_TAG=${CIRCLE_WORKFLOW_ID:0:7}
            
  scan-frontend:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - run:
          name: Install Trivy
          command: |
            wget https://github.com/aquasecurity/trivy/releases/download/v0.20.0/trivy_0.20.0_Linux-64bit.tar.gz
            tar zxvf trivy_0.20.0_Linux-64bit.tar.gz
            sudo mv trivy /usr/local/bin/
      - run:
          name: Scan Frontend Docker Image
          command: trivy ${DOCKER_REPO}/web:${CIRCLE_WORKFLOW_ID:0:7}

  scan-backend:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - run:
          name: Install Trivy
          command: |
            wget https://github.com/aquasecurity/trivy/releases/download/v0.20.0/trivy_0.20.0_Linux-64bit.tar.gz
            tar zxvf trivy_0.20.0_Linux-64bit.tar.gz
            sudo mv trivy /usr/local/bin/
      - run:
          name: Scan Backend Docker Image
          command: trivy ${DOCKER_REPO}/api:${CIRCLE_WORKFLOW_ID:0:7}

  deploy-eks:
    docker:
      - image: hashicorp/terraform:1.5
    steps:
      - checkout
      - install_awscli
      - run:
          name: Configure AWS CLI
          command: |
            aws configure set aws_access_key_id ${AWS_ACCESS_KEY} --profile default
            aws configure set aws_secret_access_key ${AWS_SECRET_KEY} --profile default
            aws configure set default.region ${aws_region} --profile default
            aws configure set default.output csv --profile default
      - run:
          name: Provision EKS Resources
          command: |
            cd terraform-eks
            terraform init

            if aws s3 ls "s3://${STATE_S3_BUCKET}" 2>&1 | grep -q 'NoSuchBucket'; then
              echo "Bucket does not exist. Aborting EKS deployment..."
              exit 1
            else
              echo "Bucket exists. Proceeding with deployment..."  
              terraform apply -auto-approve -var-file="vars.tfvars" -var="user_access_key=${AWS_ACCESS_KEY}" \
                              -var="user_secret_key=${AWS_SECRET_KEY}" -var="region=${aws_region}" \
                              -var="state_s3_bucket=${STATE_S3_BUCKET}" -var="state_file_path=${STATE_FILE_PATH}" \
                              -var="k8s_cluster_name=${eks_cluster}"
            fi

      - destroy-eks

  deploy-frontend:
    docker:
      - image: hashicorp/terraform:1.5
    steps:
      - checkout
      - install_awscli
      - run:
          name: Configure AWS CLI
          command: |
            aws configure set aws_access_key_id ${AWS_ACCESS_KEY} --profile default
            aws configure set aws_secret_access_key ${AWS_SECRET_KEY} --profile default
            aws configure set default.region ${aws_region} --profile default
            aws configure set default.output csv --profile default
      - run:
          name: Update kubectl config with EKS cluster
          command: |
            if aws eks --region ${aws_region} describe-cluster --name ${eks_cluster} 2>&1 | grep -q 'error'; then
              echo "Error: EKS cluster does not exist or there was an error."
              exit 1
            else
              echo "EKS cluster exists and is active. Updating kubectl config..."
              aws eks --region ${aws_region} update-kubeconfig --name ${eks_cluster}
            fi
      - run:
          name: Verify connection to EKS
          command: kubectl get nodes
      - run:
          name: Update the frontend deployment script with the latest image
          command: |
            yq e -i '.spec.template.spec.containers[0].image = "${DOCKER_REPO}/web:${CIRCLE_WORKFLOW_ID:0:7}"' ./kubernetes/web-deployment.yaml
      - run:
          name: Deploy Application manifests 
          command: |
            if ! kubectl get ns ${app_namespace}_frontend > /dev/null 2>&1; then
              kubectl create ns ${app_namespace}_frontend
            fi

            kubectl apply -n ${app_namespace}_frontend  -f kubernetes/
            frontend_app_name=$(kubectl ${app_namespace}_frontend get deployments -o=jsonpath='{.items[0].metadata.name}')
            kubectl -n ${app_namespace}_frontend rollout status deployment/frontend_app_name
      - run:
          name: Wait for 1 minute
          command: sleep 60
      - run:
          name: Verify the deployment
          command: |
            kubectl get pods,svc -n ${app_namespace}
            
      - destroy-environment
  
  deploy-backend:
    docker:
      - image: hashicorp/terraform:1.5
    steps:
      - checkout
      - install_awscli
      - run:
          name: Configure AWS CLI
          command: |
            aws configure set aws_access_key_id ${AWS_ACCESS_KEY} --profile default
            aws configure set aws_secret_access_key ${AWS_SECRET_KEY} --profile default
            aws configure set default.region ${aws_region} --profile default
            aws configure set default.output csv --profile default
      - run:
          name: Update kubectl config with EKS cluster
          command: |
            if aws eks --region ${aws_region} describe-cluster --name ${eks_cluster} 2>&1 | grep -q 'error'; then
              echo "Error: EKS cluster does not exist or there was an error."
              exit 1
            else
              echo "EKS cluster exists and is active. Updating kubectl config..."
              aws eks --region ${aws_region} update-kubeconfig --name ${eks_cluster}
            fi
      - run:
          name: Verify connection to EKS
          command: kubectl get nodes
      - run:
          name: Update the backend deployment script with the latest image
          command: |
            yq e -i '.spec.template.spec.containers[0].image = "${DOCKER_REPO}/api:${CIRCLE_WORKFLOW_ID:0:7}"' ./kubernetes/api-deployment.yaml
      - run:
          name: Deploy Application manifests 
          command: |
            if ! kubectl get ns ${app_namespace}_backend > /dev/null 2>&1; then
              kubectl create ns ${app_namespace}_backend
            fi

            kubectl apply -n ${app_namespace}_backend  -f kubernetes/
            backend_app_name=$(kubectl ${app_namespace}_backend get deployments -o=jsonpath='{.items[0].metadata.name}')
            kubectl -n ${app_namespace}_backend rollout status deployment/backend_app_name
      - run:
          name: Wait for 1 minute
          command: sleep 60
      - run:
          name: Verify the deployment
          command: |
            kubectl get pods,svc -n ${app_namespace}
            
      - destroy-environment

  clean-up:
    docker:
      - image: hashicorp/terraform:1.5
    steps:
      - checkout
      - install_awscli
      - run:
          name: Trigger Cleanup
          command: exit 1
            
      - destroy-environment
          
workflows:
  dev:
    jobs:
      - build-frontend
      - build-backend
      - scan-frontend:
          requires: [build-frontend]
      - scan-backend:
          requires: [build-backend]
      - deploy-eks:
          requires: [scan-frontend, scan-backend]
          parameters:
            aws_region: <<parameters.aws_region>>
          filters:
            branches:
              only: [main]
      - deploy-frontend:
          requires: [deploy-eks]
          parameters:
            app_namespace: <<parameters.app_namespace>>
            eks_cluster: <<parameters.eks_cluster>>
            aws_region: <<parameters.aws_region>>
      - deploy-backend:
          requires: [deploy-eks]
          parameters:
            app_namespace: <<parameters.app_namespace>>
            eks_cluster: <<parameters.eks_cluster>>
            aws_region: <<parameters.aws_region>>
      # - cleanup:
      #     requires: [deploy-backend, deploy-frontend]
            # parameters:
            # app_namespace: <<parameters.app_namespace>>
            # eks_cluster: <<parameters.eks_cluster>>
            # aws_region: <<parameters.aws_region>>